"""æµ‹è¯•GPUåŠ é€Ÿ"""
import torch
import time

print("="*60)
print("GPUæ€§èƒ½æµ‹è¯• - RTX 5070")
print("="*60)

# æ£€æŸ¥CUDA
print(f"\nğŸ“¦ PyTorchä¿¡æ¯:")
print(f"  ç‰ˆæœ¬: {torch.__version__}")
print(f"  CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"  cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    print(f"\nğŸ® GPUä¿¡æ¯:")
    print(f"  GPUæ•°é‡: {torch.cuda.device_count()}")
    print(f"  å½“å‰GPU: {torch.cuda.get_device_name(0)}")
    props = torch.cuda.get_device_properties(0)
    print(f"  æ˜¾å­˜æ€»é‡: {props.total_memory / 1024**3:.2f} GB")
    print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    print(f"  å¤šå¤„ç†å™¨æ•°é‡: {props.multi_processor_count}")
    
    # æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    print(f"\nğŸ’¾ æ˜¾å­˜ä½¿ç”¨:")
    print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
    print(f"  å·²ç¼“å­˜: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB")
    
    # æ€§èƒ½æµ‹è¯•
    print(f"\nâš¡ æ€§èƒ½æµ‹è¯•:")
    device = torch.device("cuda")
    
    # å°çŸ©é˜µä¹˜æ³•ï¼ˆçƒ­èº«ï¼‰
    print("  çƒ­èº«ä¸­...")
    a = torch.randn(1000, 1000, device=device)
    b = torch.randn(1000, 1000, device=device)
    _ = torch.matmul(a, b)
    torch.cuda.synchronize()
    
    # å¤§çŸ©é˜µä¹˜æ³•æµ‹è¯•
    sizes = [5000, 10000, 15000]
    for size in sizes:
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        
        start = time.time()
        c = torch.matmul(a, b)
        torch.cuda.synchronize()
        elapsed = time.time() - start
        
        gflops = (2 * size**3) / (elapsed * 1e9)
        print(f"  {size}x{size} çŸ©é˜µä¹˜æ³•: {elapsed:.4f}ç§’ ({gflops:.2f} GFLOPS)")
    
    # æµ‹è¯•æ‰¹é‡æ“ä½œ
    print(f"\nğŸ”¥ æ‰¹é‡å¤„ç†æµ‹è¯•:")
    batch_size = 256
    input_size = 1000
    hidden_size = 512
    
    x = torch.randn(batch_size, input_size, device=device)
    w1 = torch.randn(input_size, hidden_size, device=device)
    w2 = torch.randn(hidden_size, hidden_size, device=device)
    
    start = time.time()
    for _ in range(100):
        h = torch.relu(torch.matmul(x, w1))
        out = torch.matmul(h, w2)
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    print(f"  100æ¬¡å‰å‘ä¼ æ’­ (batch={batch_size}): {elapsed:.4f}ç§’")
    print(f"  å¹³å‡æ¯æ¬¡: {elapsed/100*1000:.2f}ms")
    
    # æ¸…ç†
    del a, b, c, x, w1, w2, h, out
    torch.cuda.empty_cache()
    
    print(f"\nâœ… GPUåŠ é€Ÿæ­£å¸¸å·¥ä½œï¼")
    print(f"   ä½ çš„RTX 5070æ€§èƒ½å¾ˆå¼ºï¼Œéå¸¸é€‚åˆè®­ç»ƒæ·±åº¦RLç®—æ³•ï¼")
    
else:
    print("\nâŒ CUDAä¸å¯ç”¨")
    print("   è¯·æ£€æŸ¥:")
    print("   1. nvidia-smiå‘½ä»¤æ˜¯å¦æ­£å¸¸")
    print("   2. PyTorchæ˜¯å¦å®‰è£…äº†GPUç‰ˆæœ¬")
    print("   3. CUDAé©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")

print("\n" + "="*60)

